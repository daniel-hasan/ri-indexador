{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta prática você irá implementar o indexador para, logo após, indexar o conteúdo da Wikipédia. Fizemos uma implementação inicial na qual o índice é composto pela classe abstrata `Index` que armazena a estrutura do índice e possui as operações básicas do mesmo. \n",
    "\n",
    "Iremos fazer duas implementações desse índice: o `HashIndex` que será um índice simples em memória principal e o `FileIndex` em que as ocorrências ficarão em memória secundária para possibilitar a indexação de uma quantidade maior de páginas. Assim, teremos os seguintes arquivos:\n",
    "\n",
    "- `structure.py`: Possui toda a estrutura do índice;\n",
    "- `index_structure_test.py`: Testa a estrutura do índice;\n",
    "- `file_index_test.py`: Possui os testes unitários específicos para a indexação das ocorrencias em arquivos da classe `FileIndex`;\n",
    "- `performance_test.py`: Executa um teste de performance (tempo de execução e memória utilizada) do índice;\n",
    "- `indexer.py`: Possui as classes para o preprocessamento e preparação para a indexação;\n",
    "- `indexer_test.py`: Realiza o [teste de integração](https://en.wikipedia.org/wiki/Integration_testing) da prática como um todo (inclusive as funções da indexer.py).\n",
    "\n",
    "**Na entrega, não esqueça de apresentar a saída de execução de cada atividade desta tarefa.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação da classe Abstrata `Index` e classe `HashIndex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe `Index` é abstrata e possui os métodos para manipulação do indices além da estrutura do índice que é comum em todas as estruturas que ficará sempre em memória principal durante sua execução:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dic_index`:  dicionário em que a chave é o termo indexação (string, gerenciado por esta classe) e, os valores,  podem der de diferentes tipos - dependendo da subclasse;\n",
    "- `set_documents`: conjunto de ids de documentos existentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teremos duas subclasses a `HashIndex` e a `FileIndex` que serão responsáveis pela implementação dos métodos e criação da estrutura - manipulando o valor do atributo `dic_index`. Os métodos e as demais classes deste arquivo serão discutidos ao longo das atividades. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - método index da classe Index**: Este método está quase todo pronto e é reponsável por indexar um termo com sua frequencia e documento no índice, de acordo com uma de suas subclasses. Nesse método você deverá inicializar a variável `int_term_id` apropriadamente - substituindo os `None` correspondente: \n",
    "\n",
    "- **Caso o termo não exista no índice**, deverá obter o próximo term_id. Esse id pode ser sequencial - verificando o tamanho do vocabulário por meio do atributo `dict_index`. \n",
    "- **Caso esse termo seja encontrado**, a classe Index deverá chamar o método `get_term_id` (método abstrato da classe `Index` implementado pelas subclasses) para obtê-lo pois, dependendo da implementação, haverá uma forma diferente de obtenção. \n",
    "\n",
    "Logo após, você deverá atualizar o atributo `set_documents` apropriadamente com o novo documento encontrado. Para testar tanto esta atividade e a seguinte,  você deverá fazer uma das implementações dessa classe - a classe **HashIndex** - na atividade 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2- atributos calculados da classe Index**: Por meio dos atributos existentes na classe `Index`, implemente os atributos calculados `document_count` e `vocabulary` da classe Index: \n",
    "- O atributo `document_count` retorna a quantidade de documentos existentes (inteiro); \n",
    "- `vocabulary` retorna uma lista com o vocabulário completo indexado (lista de string). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - Implementação da classe HashIndex:** O HashIndex deverá fazer um índice em memória.\n",
    "\n",
    "Como exemplo, caso tenhamos três documentos $d_1 = $\"A casa verde é uma casa bonita\", $d_2 = $\"A casa bonita\" e $d_3 =$\"O prédio verde\", caso não haja remoção de _stopwords_ nem acentos, o atributo `dic_index` deverá possuir a seguinte estrutura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [(term_id:1 doc: 1 freq: 1), (term_id:1 doc: 2 freq: 1)],\n",
       " 'casa': [(term_id:2 doc: 1 freq: 2), (term_id:2 doc: 2 freq: 1)],\n",
       " 'verde': [(term_id:3 doc: 1 freq: 1), (term_id:3 doc: 3 freq: 1)],\n",
       " 'é': [(term_id:4 doc: 1 freq: 1)],\n",
       " 'uma': [(term_id:5 doc: 1 freq: 1)],\n",
       " 'bonita': [(term_id:6 doc: 1 freq: 1), (term_id:6 doc: 2 freq: 1)],\n",
       " 'o': [(term_id:7 doc: 3 freq: 1)],\n",
       " 'prédio': [(term_id:8 doc: 3 freq: 1)]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from index.structure import *\n",
    "{\"a\": [TermOccurrence(1,1,1), TermOccurrence(2,1,1)],\n",
    " \"casa\": [TermOccurrence(1,2,2), TermOccurrence(2,2,1)],\n",
    " \"verde\": [TermOccurrence(1,3,1), TermOccurrence(3,3,1)],\n",
    " \"é\": [TermOccurrence(1,4,1)],\n",
    " \"uma\": [TermOccurrence(1,5,1)],\n",
    " \"bonita\": [TermOccurrence(1,6,1),TermOccurrence(2,6,1)],\n",
    " \"o\": [TermOccurrence(3,7,1)],\n",
    " \"prédio\": [TermOccurrence(3,8,1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por simplicidade deste índice, perceba que deixamos o `term_id` de forma repetida. Iremos deixar assim, porém poderíamos retirar essa redundância para reduzir o consumo de memória. que deixamos o `term_id` de forma repetida. Iremos deixar assim, porém poderiamos retirar essa redundancia para reduzir o consumo de memória. Mesmo assim, lembre-se que, nesta prática, iremos implementar o índice em arquivo que irá ser melhor ainda na questão de consumo de memória ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O índice é chamado da seguinte forma — esse código só ira funcionar depois que você terminar esta atividade :):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index.structure import *\n",
    "index = HashIndex()\n",
    "#indexação do documento 1\n",
    "index.index(\"a\",1,1)\n",
    "index.index(\"casa\",1,2)\n",
    "index.index(\"verde\",1,1)\n",
    "index.index(\"é\",1,1)\n",
    "index.index(\"uma\",1,1)\n",
    "#indexação do documento 2\n",
    "index.index(\"a\",2,1)\n",
    "index.index(\"casa\",2,1)\n",
    "index.index(\"bonita\",2,1)\n",
    "\n",
    "#indexação do documento 3\n",
    "index.index(\"o\",3,1)\n",
    "index.index(\"prédio\",3,1)\n",
    "index.index(\"verde\",3,1)\n",
    "\n",
    "index.finish_indexing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o método `index` e `finish_indexing` são da classe abstrata, ou seja, funcionarão para qualquer estrutura de indice. A classe `Index` é que irá manter o dicinário com o vocabulário do índice e suas ocorrencias serão armazenadas de forma diferente, de acordo com a implementação das subclasses:\n",
    "- A classe `HashIndex`, irá armazenar o indice e suas ocorrencias em memória principal \n",
    "- A classe `FileIndex` armazenará as ocorrências em arquivo, ambas subclasses de `Index`. \n",
    "\n",
    "O método `finish_indexing` é um método que não está implementado no `Index` e é implementado (opcionalmente) nas suas subclasses caso haja necessidade de fazer algo no final da indexação. Em nosso caso, apenas a classe `FileIndex` irá precisar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, você deverá implementar a classe HashIndex. Para isso, você deverá completar os métodos `create_index_entry` e `add_index_occur` que são bem simples. Esse métodos são responsáveis, respectivamente, por criar uma entrada no dicionário `dic_index` e adicionar mais uma ocorrencia nele.  Tais métodos foram criados para deixarmos em responsabilidade das subclasses a manipulação dessa nova entrada, pois, dependendo da estrutura do índice, ela será diferente. Implemente de acordo com a descrição abaixo e veja também o método `index` da classe `Index` para entender melhor como esses métodos são usados.\n",
    "- `create_index_entry`: cria uma nova entrada no índice utilizando, se necessário, o id do termo passado como parâmetro - esse id não será necessário para a HashIndex. No caso dessa classe, a implementação deste método é **super simples** - apenas substitua o None por uma lista vazia;\n",
    "- `add_index_occur`: Adiciona uma nova ocorrencia   neste índice. Você terá como entrada: o termo desta ocorrencia, o id do documento e frequencia do termo no documento. Você deverá instanciar um objeto da classe `TermOccurrence` substituindo o `None` apropriadamente. \n",
    "\n",
    "\n",
    "Faça os testes abaixo para garantir que a atividade atual e as duas anteriores foram implementadas corretamente. Nos primeiros dois testes, você ainda não verá a lista de ocorrências, pois o método para obtê-la será implementado a seguir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.index_structure_test StructureTest.test_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.index_structure_test StructureTest.test_document_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, implemente o método `get_occurrence_list`: Retornará a lista de ocorrencias de um determinado termo. Considerando o exemplo apresentado no inicio desta atividade, `index.get_occurrence_list('casa')` retornará a lista `[TermOccurrence(1,2,2), TermOccurrence(2,2,1)]`. Caso um termo não exista, este método deverá retornar uma lista vazia. Logo após, execute o teste abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.index_structure_test StructureTest.test_get_occurrence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemente o atributo calculado `document_count_with_term` que retorna a quantidade de documentos que possuem um determinado termo. Considerando o exemplo apresentado no inicio desta atividade, `index.document_count_with_term('casa')` retornará 2. Caso um termo não exista, este método deverá retornar zero. Logo após, execute o teste abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.index_structure_test StructureTest.test_document_count_with_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 4 — métodos de comparação da classe TermOccurrence**: Eventualmente iremos precisar ordenar as ocorrências Por isso, temos que implementar os [comparadores de `__eq__` e `__lt__`](https://docs.python.org/3.7/reference/datamodel.html#object.__lt__) além de usar o _decorator_ [total_ordering](https://docs.python.org/3.7/library/functools.html#functools.total_ordering) - [veja também aqui](https://portingguide.readthedocs.io/en/latest/comparisons.html#rich-comparisons). O método `__eq__` retorna igual se um objeto é considerado igual ao outro. Considere que uma ocorrência é igual a outra se o id do termo dela e o id do documento forem iguais.\n",
    "\n",
    "O comparador `<` é implementado pelo método `__lt__` que retorna verdadeiro se o objeto corrente `self` é menor que o objeto passado como parâmetro. A ocorrência deverá ser ordenada primeiramente pelo seu `term_id` e, logo após, pelo `doc_id`. Faça o exemplo abaixo para testar (não esqueça de reiniciar o Kernel quando modificar o código):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index.structure import *\n",
    "t1 = TermOccurrence(1,1,2)\n",
    "t2 = TermOccurrence(3,1,2)\n",
    "t3 = TermOccurrence(1,2,2)\n",
    "t4 = TermOccurrence(2,2,2)\n",
    "t5 = TermOccurrence(2,2,2)\n",
    "\n",
    "\n",
    "print(f\"Resultado obtido: {t1 == t5} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t4 == t5} - esperado: True\")\n",
    "print(f\"Resultado obtido: {t1 != t1} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t1 is None} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t1 < t2} - esperado: True\")\n",
    "print(f\"Resultado obtido: {t2 > t3} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t3 < t4} - esperado: True\")\n",
    "print(f\"Resultado obtido: {t2 > t4} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t2 > None} - esperado: False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, você implementou todos os métodos para indexação usando memória principal por meio da classe `HashIndex`. Abaixo, \"brinque\" com os métodos \"index\" e, logo após, testar o resultado dos métodos implementados (tanto de retornar uma lista de ocorrencia quanto de verificar a quantidade de documentos com um determinado termo). Dentre os termos a serem indexados, indexe os sete últimos digitos do número de matrícula de cada integrante do grupo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção de índice usando arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A construção de índice usando apenas memória principal é fácil de implementar e eficiente em termos de tempo de execução. Porém, quando precisamos de indexar milhões/bilhões de páginas, é muitas vezes inviável armazenarmos tudo em memória principal. \n",
    "\n",
    "Para resolver esse problema, uma solução é mantermos o vocabulário em memória principal e as ocorrências em memória secundária. Assim, teríamos o mesmo atributo `dic_index` na classe `Index`. Porém, cada entrada (termo) referenciará as ocorrencias em arquivo. Utilizando exemplo da `atividade 3`neste contexto, no final da indexação, o `dic_index` deve ficar da seguinte forma: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"a\": TermFilePosition(term_id=1,  term_file_start_pos=0, doc_count_with_term=2), \n",
    " \"casa\": TermFilePosition(2, 20, 2), \n",
    " \"verde\": TermFilePosition(3, 40, 2),\n",
    " \"é\": TermFilePosition(4, 60, 1), \n",
    " \"uma\": TermFilePosition(5, 70, 1), \n",
    " \"bonita\": TermFilePosition(6, 80, 2), \n",
    " \"o\": TermFilePosition(7, 100, 1), \n",
    " \"prédio\": TermFilePosition(8, 110, 1), \n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ocorrências são ordenadas por termo e, logo após, por documento. Dessa forma, elas ficariam em um arquivo na seguinte ordem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[TermOccurrence(1,1,1), \n",
    " TermOccurrence(2,1,1), \n",
    " TermOccurrence(1,2,2), \n",
    " TermOccurrence(2,2,1),\n",
    " TermOccurrence(1,3,1), \n",
    " TermOccurrence(3,3,1),\n",
    " TermOccurrence(1,4,1),\n",
    " TermOccurrence(1,5,1),\n",
    " TermOccurrence(1,6,1),\n",
    " TermOccurrence(2,6,1),\n",
    " TermOccurrence(3,7,1),\n",
    " TermOccurrence(3,8,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em que cada instância da classe `TermFilePosition` é a especificação da posição inicial de um `term_id` em um arquivo além de especificar também a quantidade de ocorrências desse termo. Essa posição inicial e quantidade são definidas nos atributos `term_file_start_pos` e `doc_count_with_term`, respetivamente. A posição inicial está em ‘bytes’ e, nesse exemplo, foi considerado que cada TermOcurrence possui 10 ‘bytes’. Assim, por exemplo, o termo casa (term_id=2) inicia-se na posição 20 e possui duas ocorrências. Com isso, é possível obter todas as ocorrências de um determinado termo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para deixarmos a estrutura dessa forma, temos um dificultador: no arquivo, temos que ordenar as ocorrencias pelo termo, porém, indexamos por documento. Assim, se gravássemos as ocorrências assim que indexarmos, as gravariamos agrupadas por documento (veja na atividade 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, temos que garantir uma ordenação por termo do arquivo externo, lembrando que nem sempre é possível armazenar todo o arquivo em memória principal. Para resolvermos isso, faremos o seguinte:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sempre, ao indexar, salvaremos o indice em uma lista (temporária) de ocorrencia de termos `lst_occurrences_tmp`\n",
    "- Usaremos o método `save_tmp_occurrences` para, assim que a lista estiver com um determinado tamanho, ordená-la pelo termo e salvar de formar ordenada em um novo arquivo de indice com a todas as ocorrencias. Para que seja feito isso, você deverá fazer uma ordenação externa lendo em consideração o índice em arquivo atual e a lista de ocorrencias temporárias. Veja a seguir o passo a passo geral da indexação. As proximas atividades irão te guiar para que seja implementado:\n",
    "    - (1) ordene a lista `lst_occurrences_tmp`. Lembre-se que você implementou os comparadores das instâncias TermOccurrence, assim, a ordenação e descobrir o menor valor entre as ocorrencias é uma operação simples;\n",
    "    - (2) criar um arquivo novo;\n",
    "    - (3) compare a primeira posição da lista com a primeira posição do arquivo de índice, sempre inserindo a ocorrencia considerada com o menor entre elas no novo arquivo. Lembrando novamente que os comparadores foram implementados e que você possui os métodos `next_from_list` e `next_from_file` - que será implementado na atividade 5 para ajudar;\n",
    "    - (4) esse novo arquivo passará a ser o índice. Exclua o indice antigo e limpe a lista de ocorrências `lst_occurrences_tmp`.\n",
    "- O método `finish_indexing` é o método que será chamado ao finalizar a indexação. Neste contexto ele será usado para organizar o `dic_index`. Lembrando que, neste contexto, `dic_index` mapeia uma palavra (string) a uma instancia de `TermFilePosition`, este método irá atualizar  os atributos `term_file_start_pos` e `doc_count_with_term` de cada instancia  de `TermFilePosition` para os valores corretos, considerando que termo ele se refere no arquivo do índice.\n",
    "\n",
    "Na primeira execução, não haverá arquivo e você adicionará a lista toda no arquivo de forma sequencial. Fazendo esse procedimento, você sempre irá garantir um arquivo ordenado da forma esperada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5(a) — escrita e leitura das ocorrencias em arquivo:** Iremos necessitarda leitura e escrita de uma instância de `TermOccurrence` que está persistida em arquivo. Assim deveremos implementar o método de escrita em arquivo nessa classe. Para economizar espaço e por simplicidade, será escrito num arquivo binário armazenando os três atributos inteiros. Cada inteiro será armazenado em 4 ‘bytes’ — será o suficiente para a nossa indexação. Veja um exemplo abaixo de escrita, leitura e impressão do posicionamento no arquivo (esses métodos serão úteis nessa atividade)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "número: 100\n"
     ]
    }
   ],
   "source": [
    "x = 100\n",
    "with open(\"xuxu.idx\",\"wb\") as file:\n",
    "    print(file.tell())\n",
    "    file.write(x.to_bytes(4,byteorder=\"big\"))\n",
    "    print(file.tell())\n",
    "with open(\"xuxu.idx\",\"rb\") as file:\n",
    "    print(f\"número: {int.from_bytes(file.read(4),byteorder='big')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, implemente: \n",
    "\n",
    "- o método `write` da classe `TermOccurence` responsável por gravar os atributos `doc_id`, `term_id` e `term_freq` do objeto corrente.\n",
    "- complete o método `next_from_file` da classe `FileIndex`. Este método já possui um arquivo aberto e você deverá ler o próximo objeto da classe `TermOccurence` que foi escrito neste arquivo pelo método `write` implementado acima. Caso não haja mais ocorrencia, é retornado `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.file_index_test FileIndexTest.test_next_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5 (b) - leitura e obtenção do próximo elemento da lista**: Conforme dito, iremos usar uma lista de ocorrencias em memória principal para, logo após, armazenar cada ocorrência, de forma ordenada em arquivo. Essa lista de ocorrencia é o atributo `lst_occurrences_tmp`. Dessa forma, temos que implementar os métodos da classe `FileIndex`:\n",
    "\n",
    "- `create_index_entry`: instancia um elemento da classe `FilePosition` com o seu respectivo id do termo, passado como parâmetro\n",
    "- `add_index_occur`: Cria e adiciona uma nova ocorrência na lista de ocorrencias temporárias `lst_occurrences_tmp` e, caso senha passado o limite `FileIndex.TMP_OCCURRENCES_LIMIT` do número máximo de ocorrencias na lista, chamar o método `save_tmp_occurrence` (ainda será implementado). \n",
    "- `next_from_list`: Retorna o primeiro elemento da lista, eliminando-o da lista. Caso não exista, retorna `None`. Este método é útil para obter o menor elemento da lista, quando a mesma estiver ordenada de forma crescente (o que irá ocorrer em `save_tmp_occurences`). \n",
    "\n",
    "Poderíamos implementar esses dois métodos de forma simples utilizando as funções `append` e `pop` da lista. Porém, a complexidade de tempo no pior caso no `append` é O(N) em tempo e espaço (quando o tamanho da lista exceder o tamanho anteriormente alocado) e o `pop` a complexidade de tempo é O(N) para retirar o primeiro elemento da lista. Dessa forma, como iremos fazer uma lista com 1 milhão de ocorrêcias que irão ser modificados constantemente, essas operações seriam muito custoas. \n",
    "\n",
    "Uma das soluções para contornar esse problema é utilizamos uma lista fixa de  tamanho `TMP_OCCURRENCES_LIMIT` e variáveis auxiliares para indicar o início e fim da lista. Dessa forma, nesta classe, possuímos os atributos `idx_tmp_occur_first_element` e `idx_tmp_occur_last_element` para indicar a posição do primeiro e ultimo item válido na lista. A lista será inicializada no construtor com `TMP_OCCURRENCES_LIMIT` instancias `None`. A inicialização desses atributos já foi implementada no construtor. Implementamos o método `get_tmp_occur_size` para obter o tamanho dessa lista. Como alternativa a essa solucação, poderiamos usar uma [lista encadeada (deque)](https://docs.python.org/3/library/collections.html#collections.deque). Mas, nessa atividade, você não poderia fazer com a lista encadeada apenas porque os testes automatizados não funcionariam ;-). Complete a implementação dos métodos `add_index_occur` e o método `next_from_list` apropriadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.file_index_test FileIndexTest.test_next_from_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 6 - método save_tmp_occurrences:** Implemente o método `save_tmp_occurrences`. Esse método deverá fazer a junção do índice atual e a lista `lst_occurrences_tmp` em novo arquivo de índice de forma ordenada, conforme explicado anteriormente no início desta seção. Neste método, você não precisa preocupar com o atributo `dic_index`. Além da lista, leve em consideração os seguintes atributos/métodos:\n",
    "\n",
    "- `idx_file_counter`:  No código, você irá criar sempre novos indices, excluindo o antigo. Este atributo será útil para definirmos o nome do arquivo do índice. O novo arquivo do índice chamará `occur_index_X` em que $X$ é o número do mesmo. \n",
    "- `str_idx_file_name`: Atributo que armazena o arquivo índice atual. A primeira vez que executarmos `save_tmp_occurrences` não haverá arquivo criado e, assim `str_idx_file_name = None`\n",
    "- `next_from_file` e `next_from_list`: implementados na atividade anterior, para obter o próximo item do arquivo ou da lista. Lembre-se que a lista deve estar ordenda antes de juntar a lista e o arquivo de indice atual em um novo arquivo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o teste unitário abaixo para verificar corretude deste código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.file_index_test FileIndexTest.test_save_tmp_occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 7: método finish_indexing:** Para que o `dic_index` encontre as ocorrencias no arquivo, para cada termo, é mapeado o `TermFilePosition` correspondente. Agora, com as ocorrências organizadas no arquivo por termo, você deverá implementar o método `finish_indexing` para atualizar o atributo `dic_index` colocando, para cada instancia de `TermFilePosition` a posição inicial e quantidade de documentos de cada termo`. Logo após, execute o teste unitário abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.file_index_test FileIndexTest.test_finish_indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 8 - implementação em `FileIndex` dos métodos abstratos da classe Index:** Como vocês perceberam, `FileIndex` é subclasse de `Index`. Assim,  precisamos implementar os métodos abstratos da classe `Index`:\n",
    "\n",
    "- `create_index_entry`: no `FileIndex` para criar uma nova entrada no índice, você deverá retornar uma instancia de `TermFilePosition` para este novo `term_id`. Ao criá-lo, você não precisa de definir a posição inicial do arquivo nem a quantidade de documentos. Conforme vocês implementaram nas atividades anteriores, isso é feito apenas no momento de finalização da indexação;\n",
    "- `get_occurrence_list` e `document_count_with_term`: Possuem as mesmas funcionalidades descritas na atividade 3, porém, agora você deverá considerar a estrutura criada no `FileIndex`. Lembrem-se que esse método é só chamado após a finalização da indexação, assim, considere que o índice já está pronto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 9 - Teste unitário** Dessa vez, você deverá alterar uma classe de teste unitário para conseguir executá-la. \n",
    "\n",
    "Agora iremos testar os métodos get_occurrence_list e document_count_with_term. Lembre-se que já temos um teste unitário para isso, porém ele testa a estrutura de um `HashIndex`. Neste caso, iremos apenas mudar a estrutura, mas os métodos serão o mesmo, por isso, conseguiremos reaproveitar o teste feito anteriormente criando apenas uma nova classe de teste. \n",
    "\n",
    "Implementando este teste, você perceberá como é lindo usar orientação objetos ao seu favor 🥰. No arquivo `index_structure_test.py` você possui a classe `FileStructureTest` que é subclasse de nosso teste criado `StructureTest`.  Você deverá implementar o método `setUp` na classe `FileStructureTest` que sobrepõe o método de mesmo nome na classe `StructureTest`. O método `setUp` é executado sempre antes do teste. Este método que você irá criar irá fazer exatamente a mesma coisa que o criado em `StructureTest` porém, você deverá instanciar um `FileIndex` ao invés de um `HashIndex`. Logo após, execute os testes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.index_structure_test  FileStructureTest.test_document_count_with_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.index_structure_test  FileStructureTest.test_get_occurrence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar, também criamos um teste de desempenho. Verifique o desempenho da indexação em arquivo e em memória ao indexar milhões de ocorrências de termos utilizando os testes abaixo. Note que, desta vez, estamos chamando os testes por comandos Python e não pelo terminal e estamos fazendo testes de memória. Dessa forma, você deve reiniciar o kernel sempre antes de executar cada um dos dois testes abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Índice completamente em memória principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from index.performance_test import PerformanceTest\n",
    "\n",
    "PerformanceTest.NUM_DOCS = 250\n",
    "PerformanceTest.NUM_TERM_PER_DOC = 1000\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(PerformanceTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Índice com ocorrências em memória secundária. Veja que, abaixo, que você pode ajustar o parâmetro de número de ocorrências em memória. Será muito útil para não gastar tanto tempo ao indexar o conteúdo da Wikipédia. **Reinicie o kernel antes de executar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from index.performance_test import FilePerformanceTest,PerformanceTest\n",
    "from index.structure import FileIndex\n",
    "\n",
    "PerformanceTest.NUM_DOCS = 250\n",
    "PerformanceTest.NUM_TERM_PER_DOC = 1000\n",
    "FileIndex.TMP_OCCURRENCES_LIMIT = 100000\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(FilePerformanceTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo após executado este teste, você deverá usar a biblioteca [JSON](https://docs.python.org/3/library/json.html) ou [Pickle](https://docs.python.org/3/library/pickle.html) para armazenar o vocabulário. Com isso, crie um método de leitura do FileIndex e do HashIndex e de escrita. O método de leitura deverá ser um método estatico que retorna um objeto da classe indice previamente criado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 10 - Gravação do índice (completo) em memória secundária:** Use o [pickle](https://docs.python.org/3/library/pickle.html) e implemente o método write da classe `Index` que grava o índice em memória secundária e o método estático `read`, que lê o índice em arquivo e o retorna. A seguir, execute os testes para leitura e escrita tanto usando o `FileIndex` quanto o `HashIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.index_structure_test StructureTest.test_read_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.index_structure_test FileStructureTest.test_read_write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexador de HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, você irá alterar o arquivo `indexer.py` para [preprocessar conteúdo HTML](https://docs.google.com/presentation/d/1C22jQWIYobiqMx8SmP1y2lr1uSlvJSu3ayu5lXC5d8A/edit?usp=sharing) e depois indexá-lo. Com isso, você poderá usá-lo para indexação das páginas HTML, como os da Wikipédia. A classe `Cleaner` será responsável pelo preprocessamento e a HTMLIndexer, para a indexação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index.indexer import *\n",
    "#importamos o módulo structure\n",
    "#novamente para não precisar de executar o código do início da tarefa\n",
    "from index.structure import *\n",
    "#Fazemos o download do módulo nlkt para a questão 12\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 11 - Limpeza dos dados com a classe Cleaner:** A classe `Cleaner` é responsável por preprocessar o conteúdo HTML para que ele esteja preparado para indexação. Essa classe tem alguns _flags_ para definir se algum tipo de processamento opcional será feito (por exemplo, _stemming_ e remoção de _stopwords_). Para isso, você deverá implementar pequenos métodos para fazer a limpeza. Esses códigos são pequenos pois temos lindas APIs para nos ajudar 💕. Você irá fazer o processamento básico e, se quiser, pode melhorar a implementação criando exceções na remoção de acentos e não retirando maiúsculas e minúsculas de certas palavras e unindo palavras compostas, por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada tarefa, há um método para ser criado a seguir os testes iniciais serão feitos aqui no Jupyter. Não esqueça de reiniciar o kernel sempre que alterar algo no código. Logo após, haverá um [teste de integração](https://en.wikipedia.org/wiki/Integration_testing) para avaliar a indexação como um todo.\n",
    "\n",
    "- **Transformação de HTML para texto:** Na limpeza dos dados, iremos remover tudo que não será indexado — ou seja, o código HTML. Para isso, iremos implementar o método `html_to_plain_text` que transformará o HTML em texto corrido. Você pode utilizar o [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) para isso e o método get_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_test = Cleaner(stop_words_file=\"stopwords.txt\",\n",
    "                        language=\"portuguese\",\n",
    "                        perform_stop_words_removal=True,\n",
    "                        perform_accents_removal=True,\n",
    "                        perform_stemming=True)\n",
    "cleaner_test.html_to_plain_text(\"&copy; oi! Meu nome é <strong>Hasan</strong>\")\n",
    "#esperado: '© oi! Meu nome é Hasan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Verifica se é stopword**: O método `is_stopword` retorna verdadeiro se uma palavra, passada como parâmetro, é stopword. Para isso, você irá usar o atributo `set_stop_words`. Este atributo foi inicializado com um conjunto de stopwords de um arquivo. Esse arquivo, para testes, tem poucas stopwords.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{cleaner_test.is_stop_word('japão')}, esperado: False\")\n",
    "print(f\"{cleaner_test.is_stop_word('cama')}, esperado: False\")\n",
    "print(f\"{cleaner_test.is_stop_word('é')}, esperado: True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stemming**: você deverá implementar o método word_stem para ralizar o stemming. Você deverá usar a [classe SnowballStemmer da API NLTK](https://www.nltk.org/howto/stem.html). Um objeto dessa classe já está instanciado no atributo `stemmer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{cleaner_test.word_stem('verdade')}, esperado: verdad\")\n",
    "print(f\"{cleaner_test.word_stem('estudante')}, esperado: estud\")\n",
    "print(f\"{cleaner_test.word_stem('amado')}, esperado: amad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remoção de acentos:** Iremos fazer de uma forma bem simples a remoção de acentos: aplicando uma tabela de substituição de caracteres. Para isso, você deverá criar uma [tabela de tradução](https://docs.python.org/3.3/library/stdtypes.html?highlight=maketrans#str.maketrans) no atributo `accents_translation_table` baseando-se nas variáveis `in_table` e `out_table` também presentes no construtor (substitua o None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{cleaner_test.remove_accents('canção')}, esperado: cancao\")\n",
    "print(f\"{cleaner_test.remove_accents('elétrico')}, esperado: eletrico\")\n",
    "print(f\"{cleaner_test.remove_accents('amado')}, esperado: amado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora você irá fazer o método `preprocess_word` e o `preprocess_text`:\n",
    "\n",
    "- o `preprocess_text` receberá o texto \"limpo\" (já sem código html), irá converter tudo para minuscula e remover acentos\n",
    "- o `preprocess_word` irá receber como parametro uma palavra e irá verificar se é uma palavra válida de ser indexada. Uma palavra válida a ser indexada é aquela que não é pontuação e não é stopword (caso `perform_stop_words_removal = True`). Caso não seja válida, retornará None. Caso contrário, irá retornar a palavra preprocessada. Para que seja feito o preprocessamento você deverá: fazer o stemming (se `perform_stemming = True`) - neste exemplo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 13 — Classe HTMLIndexer — método text_word_count:** Você deverá implementar o método `text_word_count`, a partir de um texto. Esse método retorna um dicionário em que, para cada palavra no texto, será apresentado sua frequência. Considere que o texto já está limpo e é necessário fazer apenas o processamento das palavras.\n",
    "\n",
    "Para isso, você deverá: dividir o texto em tokens (que, no nosso caso, são as palavras e pontuações); pré-processar cada palavra usando o `HTMLIndexer.cleaner`; e, se for uma palavra válida, contabilizá-la. Para isso, será necessário [o método word_tokenize da API NLTK](https://kite.com/python/docs/nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = HashIndex()\n",
    "indexador_teste = HTMLIndexer(index)\n",
    "indexador_teste.text_word_count(\"Olá! Qual é o dado dado que precisa?\")\n",
    "#esperado:\n",
    "#{'dad': 2, 'o': 1, 'ola': 1, 'precis': 1, 'qual': 1, 'que': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 14 — método index_text:** Implemente o método `index_text` que deverá (1) converter o HTML para texto simples usando `HTMLIndexer.cleaner`; (2) converter o texto em um dicionário de ocorrencias de palavras com sua frequencia (metodo da atividade 13); (3) indexar cada palavra deste dicionário; e (4) no final da execução, não esqueça de executar o método `finish_indexing` do índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = HashIndex()\n",
    "indexador_teste = HTMLIndexer(index)\n",
    "#o HTML está mal formado de propósito ;)\n",
    "indexador_teste.index_text(10,\"<strong>Ol&aacute;! </str> Quais são os dados que precisará?\")\n",
    "\n",
    "indexador_teste.index.dic_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esperado:\n",
    "<pre>\n",
    "{'dad': [(term_id:4 doc: 10 freq: 1)],\n",
    " 'ola': [(term_id:0 doc: 10 freq: 1)],\n",
    " 'os': [(term_id:3 doc: 10 freq: 1)],\n",
    " 'precis': [(term_id:6 doc: 10 freq: 1)],\n",
    "'qua': [(term_id:1 doc: 10 freq: 1)],\n",
    " 'que': [(term_id:5 doc: 10 freq: 1)],\n",
    " 'sao': [(term_id:2 doc: 10 freq: 1)]}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 15: Indexação de um diretório com subdiretórios** Você deverá implementar o método `index_text_dir` que, dado um diretório, navega em todos os seus subdiretórios e indexa todos os arquivos HTMLs. Considere que os arquivos sejam sempre nomeados pelo seu ID. Veja o exemplo em `doc_test`. Logo após, execute o teste unitário para ver a corretude do seu indexador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.indexer_test IndexerTest.test_indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 16 indexação dos artigos da Wikipedia:** Você deverá indexar todos os artigos da Wikipédia que estão [neste repositório](https://github.com/daniel-hasan/ri-tp-wiki-data/archive/refs/heads/master.zip) e logo após, salve esse índice. Você usará ele para a próxima etapa do projeto, o processamento de consultas. \n",
    "\n",
    "Como são dezenas de milhares de artigos, é uma tarefa que consome muita memória e **irá demorar horas**. Dessa forma, use o arquivo python `wikipedia_index.py` e execute esse arquivo no terminal. Salve esse indice como `wiki.idx`. Logo após, execute o teste abaixo que irá ler este índice e verificar se os documentos foram salvos.\n",
    "\n",
    "Nesse indice, para passar nos testes, use a configuração do `Cleaner` definida no `wikipedia_index.py`. Logo após, você pode criar outros índices. Inclusive, para a proxima etapa do projeto, aconselho que você teste diversos preprocessamentos, inclusive, você deve também achar um arquivo de stopwords com mais termos aos que feito neste teste. \n",
    "\n",
    "Você pode escolher entre o HashIndex ou FileIndex para indexar, lembrando que o HashIndex consume muito mais memória principal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.indexer_test IndexerTest.test_wiki_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a construção do relatorio, veja a especificação sobre o [guia para construção do relatório](https://docs.google.com/document/d/1spwD-rzJi3xHV8p5cIAmjSHyVEzytChuBTnEDwBaDNQ/edit#heading=h.omnow1961go)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
